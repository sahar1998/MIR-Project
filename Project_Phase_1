
from hazm import *
import csv
from nltk.stem import PorterStemmer
import xml.etree.ElementTree as ET
import nltk
from nltk.corpus import stopwords


# reading persian text from file
persian_text = ""
tree = ET.parse('Persian.xml')
root = tree.getroot()
documents_list = []
for page in root:
    page_text = ""
    for text in page[3]:
        txt = text.text
        if (txt != None):
            persian_text += txt
            page_text += txt
    documents_list.append(page_text)



# reading english text from file
english_text = ""
with open('English.csv', 'r') as f:
    reader = csv.reader(f)
    for row in reader:
        english_text += " ".join(row)


def pre_process_english(english_text):
    # case folding
    english_text = english_text.lower()

    # removing punctuations
    punctuations = '.!?,\'\"()-:;#'
    for mark in punctuations:
        english_text = english_text.replace(mark, " ")
    print(english_text)

    # tokenizing english text
    tokens = word_tokenize(english_text)
    print(tokens)

    # removing stopwords : nltk stopwords, costume stopwords

    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if not w in stop_words]

    # token_count = dict()
    # for token in tokens:
    #     if token in token_count:
    #         token_count[token] += 1
    #     else:
    #         token_count[token]= 1
    #
    # stop_words = []
    # for token in tokens:
    #     if (token_count[token] > len(tokens)/300):
    #         if token not in stop_words:
    #             stop_words.append(token)
    # print(stop_words)
    #
    # tokens=[token for token in tokens if token not in stop_words]
    print(tokens)

    # stemming
    stemmed_tokens = []
    ps = PorterStemmer()
    for token in tokens:
        stemmed_tokens.append(ps.stem(token))

    print(stemmed_tokens)

    return stemmed_tokens



def pre_process_persian(persian_text):

    # normalizing
    normalizer = Normalizer()
    persian_text = normalizer.normalize(persian_text);

    # removing punctuations
    punctuations = '.!?,\'\"()-:;#][}{*=|\\'
    for mark in punctuations:
        persian_text= persian_text.replace(mark, " ")

    # tokenizing
    tokens = word_tokenize(persian_text)

    # removing stopwords

    stop_words = stopwords_list()
    tokens = [w for w in tokens if not w in stop_words]

    # token_count = dict()
    # for token in tokens:
    #     if token in token_count:
    #         token_count[token] += 1
    #     else:
    #         token_count[token] = 1
    #
    # stop_words = []
    # for token in tokens:
    #     if token_count[token] > len(tokens) / 100:
    #         if token not in stop_words:
    #             stop_words.append(token)
    # print(stop_words)
    #
    # tokens = [token for token in tokens if token not in stop_words]
    # print(tokens)
    
    # lemmatizing
    lemmatized_tokens = []
    lemmatizer = Lemmatizer()
    for token in tokens:
        lemmatized_tokens.append(lemmatizer.lemmatize(token))

    # stemming
    stemmed_tokens = []
    stemmer = Stemmer()
    for token in tokens:
        stemmed_tokens.append(stemmer.stem(token))

    return stemmed_tokens


def spell_correction_english(input_sentence):
    input_tokens = nltk.word_tokenize(input_sentence)
    corrected = ""
    for token in input_tokens:
        bg_input_word = set(nltk.ngrams(token, n=2))
        words = nltk.corpus.words.words()
        min_jaccard = 5;
        for word in words:
            bg_word = set(nltk.ngrams(word, n=2))
            jd = nltk.jaccard_distance(bg_input_word, bg_word)
            if jd < min_jaccard:
                suggested_words = []
                min_jaccard = jd
                suggested_words.append(word)
            if jd == min_jaccard:
                suggested_words.append(word)
        print(suggested_words)
        min_edit_distance = 1000
        return_words= []
        for word in suggested_words:
            ed = nltk.edit_distance(token, word)
            if ed < min_edit_distance:
                return_words = []
                min_edit_distance = ed
                return_words.append(word)
            if ed == min_edit_distance:
                return_words.append(word)

        corrected = corrected + " " + return_words[0]
    return corrected


def spell_correction_persian(input_sentence):
    input_tokens = word_tokenize(input_sentence)
    corrected = ""
    for token in input_tokens:
        bg_input_word = bi_gram(token)
        words = words_list()
        min_jaccard = 5;
        for word in words:
            word = word[0]
            bg_word = bi_gram(word)
            jd = jaccard_distance(bg_input_word, bg_word)
            if jd < min_jaccard:
                suggested_words = []
                min_jaccard = jd
                suggested_words.append(word)
            if jd == min_jaccard:
                suggested_words.append(word)
        print(suggested_words)
        min_edit_distance = 1000
        return_words = []
        for word in suggested_words:
            ed = edit_distance(token, word)
            if ed < min_edit_distance:
                return_words = []
                min_edit_distance = ed
                return_words.append(word)
            if ed == min_edit_distance:
                return_words.append(word)
        print(return_words)
        corrected = corrected + " " + return_words[0]
    return corrected


def bi_gram(token):
    bi_grams = []
    for i in range(len(token) - 1):
        bi_grams.append(token[i:i+2])
    return bi_grams


def jaccard_distance(token1, token2):
    bi_gram_1 = bi_gram(token1)
    bi_gram_2 = bi_gram(token2)
    intersection = len([value for value in bi_gram_1 if value in bi_gram_2])
    union = len(bi_gram_1) + len(bi_gram_2) - intersection
    return 1 - (intersection / union)


def edit_distance(token1, token2):
    if len(token1) < len(token2):
        return edit_distance(token2, token1)

    if len(token2) == 0:
        return len(token1)

    previous_row = range(len(token2) + 1)
    for i, c1 in enumerate(token1):
        current_row = [i + 1]
        for j, c2 in enumerate(token2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    return previous_row[-1]

