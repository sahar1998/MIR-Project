
from hazm import *
import csv
from nltk.stem import PorterStemmer
import xml.etree.ElementTree as ET
import nltk
from nltk.corpus import stopwords
import numpy as np


# reading persian text from file
persian_text = ""
tree = ET.parse('Persian.xml')
root = tree.getroot()
persian_documents_list = []
for page in root:
    page_text = ""
    for text in page[3]:
        txt = text.text
        if (txt != None):
            persian_text += txt
            page_text += txt
    persian_documents_list.append(page_text)



# reading english text from file
english_text = ""
english_document_list = []
with open('English.csv', 'r') as f:
    reader = csv.reader(f)
    for row in reader:
        english_document_list.append(row)
        english_text += " ".join(row)
    # print(english_document_list)


def pre_process_english(english_text):
    # case folding
    english_text = english_text.lower()

    # removing punctuations
    punctuations = '.!?,\'\"()-:;#'
    for mark in punctuations:
        english_text = english_text.replace(mark, " ")
    # print(english_text)

    english_text = ''.join(i for i in english_text if not i.isdigit())

    # tokenizing english text
    tokens = word_tokenize(english_text)
    print(tokens)

    # removing stopwords : nltk stopwords, costume stopwords

    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if not w in stop_words]

    # token_count = dict()
    # for token in tokens:
    #     if token in token_count:
    #         token_count[token] += 1
    #     else:
    #         token_count[token]= 1
    #
    # stop_words = []
    # for token in tokens:
    #     if (token_count[token] > len(tokens)/300):
    #         if token not in stop_words:
    #             stop_words.append(token)
    # print(stop_words)
    #
    # tokens=[token for token in tokens if token not in stop_words]
    print(tokens)

    # stemming
    stemmed_tokens = []
    ps = PorterStemmer()
    for token in tokens:
        stemmed_tokens.append(ps.stem(token))

    print(stemmed_tokens)

    return stemmed_tokens



def pre_process_persian(persian_text):

    # normalizing
    normalizer = Normalizer()
    persian_text = normalizer.normalize(persian_text);


    # removing punctuations
    punctuations = '.!?,\'\"()-:;#][}{*=|\\'
    for mark in punctuations:
        persian_text= persian_text.replace(mark, " ")

    persian_text = ''.join(i for i in persian_text if not i.isdigit())

    # tokenizing
    tokens = word_tokenize(persian_text)

    # removing stopwords

    stop_words = stopwords_list()
    tokens = [w for w in tokens if not w in stop_words]

    # token_count = dict()
    # for token in tokens:
    #     if token in token_count:
    #         token_count[token] += 1
    #     else:
    #         token_count[token] = 1
    #
    # stop_words = []
    # for token in tokens:
    #     if token_count[token] > len(tokens) / 100:
    #         if token not in stop_words:
    #             stop_words.append(token)
    # print(stop_words)
    #
    # tokens = [token for token in tokens if token not in stop_words]
    # print(tokens)
    
    # lemmatizing
    lemmatized_tokens = []
    lemmatizer = Lemmatizer()
    for token in tokens:
        lemmatized_tokens.append(lemmatizer.lemmatize(token))

    # stemming
    stemmed_tokens = []
    stemmer = Stemmer()
    for token in tokens:
        stemmed_tokens.append(stemmer.stem(token))

    return stemmed_tokens


def spell_correction_english(input_sentence):
    input_tokens = nltk.word_tokenize(input_sentence)
    corrected = ""
    for token in input_tokens:
        bg_input_word = set(nltk.ngrams(token, n=2))
        words = nltk.corpus.words.words()
        min_jaccard = 5;
        for word in words:
            bg_word = set(nltk.ngrams(word, n=2))
            jd = nltk.jaccard_distance(bg_input_word, bg_word)
            if jd < min_jaccard:
                suggested_words = []
                min_jaccard = jd
                suggested_words.append(word)
            if jd == min_jaccard:
                suggested_words.append(word)
        print(suggested_words)
        min_edit_distance = 1000
        return_words= []
        for word in suggested_words:
            ed = nltk.edit_distance(token, word)
            if ed < min_edit_distance:
                return_words = []
                min_edit_distance = ed
                return_words.append(word)
            if ed == min_edit_distance:
                return_words.append(word)

        corrected = corrected + " " + return_words[0]
    return corrected


def spell_correction_persian(input_sentence):
    input_tokens = word_tokenize(input_sentence)
    corrected = ""
    for token in input_tokens:
        bg_input_word = bi_gram(token)
        words = words_list()
        min_jaccard = 5;
        for word in words:
            word = word[0]
            bg_word = bi_gram(word)
            jd = jaccard_distance(bg_input_word, bg_word)
            if jd < min_jaccard:
                suggested_words = []
                min_jaccard = jd
                suggested_words.append(word)
            if jd == min_jaccard:
                suggested_words.append(word)
        print(suggested_words)
        min_edit_distance = 1000
        return_words = []
        for word in suggested_words:
            ed = edit_distance(token, word)
            if ed < min_edit_distance:
                return_words = []
                min_edit_distance = ed
                return_words.append(word)
            if ed == min_edit_distance:
                return_words.append(word)
        print(return_words)
        corrected = corrected + " " + return_words[0]
    return corrected


def bi_gram(token):
    bi_grams = []
    for i in range(len(token) - 1):
        bi_grams.append(token[i:i+2])
    return bi_grams


def jaccard_distance(token1, token2):
    bi_gram_1 = bi_gram(token1)
    bi_gram_2 = bi_gram(token2)
    intersection = len([value for value in bi_gram_1 if value in bi_gram_2])
    union = len(bi_gram_1) + len(bi_gram_2) - intersection
    return 1 - (intersection / union)


def edit_distance(token1, token2):
    if len(token1) < len(token2):
        return edit_distance(token2, token1)

    if len(token2) == 0:
        return len(token1)

    previous_row = range(len(token2) + 1)
    for i, c1 in enumerate(token1):
        current_row = [i + 1]
        for j, c2 in enumerate(token2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    return previous_row[-1]


# vectorizer = TfidfVectorizer()
# vectors = vectorizer.fit_transform(english_document_list)
# feature_names = vectorizer.get_feature_names()
# dense = vectors.todense()
# denselist = dense.tolist()


def make_dict(document, unique_words):
    document_dict = dict.fromkeys(unique_words, 0)
    for word in nltk.word_tokenize(document):
        document_dict[word] += 1
    return document_dict


def compute_tf(document_dict, document):
    tfDict = {}
    document_size = len(document)
    for word, count in document_dict.items():
        tfDict[word] = count / float(document_size)
    return tfDict


def compute_idf(documents, unique_words):
    import math
    N = len(documents)
    idfDict = dict.fromkeys(unique_words, 0)
    for document_dict in documents:
        for word, val in document_dict.items():
            if val > 0:
                idfDict[word] += 1
    for word, val in idfDict.items():
        idfDict[word] = math.log(N / float(val))
    return idfDict


def compute_tfidf(document_dict, idfs):
    tfidf = {}
    for word, val in document_dict.items():
        tfidf[word] = val * idfs[word]
    return tfidf

test_document_list = ['the man went out for a walk', 'the children sat around the fire']
# print(english_document_list)


def compute_cosine(query, document_list):

    document_dict_list = []
    unique_words = []
    for document in document_list:
        document_text = document[0] + " " + document[1]
        unique_words = set(unique_words).union(set(nltk.word_tokenize(document_text)))
    unique_words = unique_words.union(set(nltk.word_tokenize(query)))

    for document in document_list:
        document_text = document[0] + " " + document[1]
        document_dict_list.append(make_dict(document_text, unique_words))

    query_tf = compute_tf(make_dict(query, set(nltk.word_tokenize(query))), query)
    query_idf = compute_idf(document_dict_list, set(nltk.word_tokenize(query)).union(unique_words))
    query_tfidf = compute_tfidf(query_tf, query_idf)

    idfs = compute_idf(document_dict_list, unique_words)

    cosine = []
    for document in document_list:
        document_text = document[0] + " " + document[1]
        similarity = 0
        tfidf = compute_tfidf(compute_tf(make_dict(document_text, unique_words), document_text), idfs)
        for word1 in query_tfidf:
            for word2 in tfidf:
                if (word1 == word2):
                    similarity += query_tfidf[word1] * tfidf[word2]
        cosine.append(similarity)

    answer = []
    k = 10
    cosine = np.array(cosine)
    ind = cosine.argsort()[-k:]  # index of the k highest elements
    for index in ind:
        answer.append(document_list[index])
    print(answer)

print(pre_process_persian(persian_text[0:10000]))



